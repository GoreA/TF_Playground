{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "VALIDATION_SIZE = 10000  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "validation_data = train_data[:VALIDATION_SIZE, ...]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:, ...]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "\n",
    "# # Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "train_data = train_data.reshape(train_data.shape[0], train_data.shape[1], train_data.shape[2], 1)\n",
    "validation_data = validation_data.reshape(validation_data.shape[0], validation_data.shape[1], validation_data.shape[2], 1)\n",
    "test_data = test_data.reshape(test_data.shape[0], test_data.shape[1], test_data.shape[2], 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "train_data = train_data.astype('float32')\n",
    "validation_data = validation_data.astype('float32')\n",
    "test_data = test_data.astype('float32')\n",
    "        \n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "train_data /= 255\n",
    "validation_data /=255\n",
    "test_data /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n",
    "    return 100.0 - \n",
    "        (100.0 * np.sum(np.argmax(predictions, 1) == labels) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map\n",
    "    # has the same size as the input). Note that {strides} is a 4D array\n",
    "    # whose shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "    # Max pooling. The kernel size spec {ksize} also follows the layout \n",
    "    # of the data. Here we have a pooling window of 2, and a stride of\n",
    "    # 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                        ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1],\n",
    "                        padding='SAME')\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape( pool, [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = NUM_EPOCHS\n",
    "train_size = train_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where training samples and labels are fed to the graph.\n",
    "# These placeholder nodes will be fed a batch of training data at each\n",
    "# training step using the {feed_dict} argument to the Run() call below.\n",
    "train_data_node = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n",
    "eval_data = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variables below hold all the trainable weights. They are passed\n",
    "# an initial value which will be assigned when we call:\n",
    "# {tf.global_variables_initializer().run()}\n",
    "conv1_weights = tf.Variable(\n",
    "    # 5x5 filter, depth 32.\n",
    "    tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  \n",
    "                        stddev=0.1,\n",
    "                        seed=SEED, dtype=tf.float32))\n",
    "conv1_biases = tf.Variable(tf.zeros([32], dtype=tf.float32))\n",
    "conv2_weights = tf.Variable(\n",
    "    tf.truncated_normal([5, 5, 32, 64], \n",
    "                        stddev=0.1,\n",
    "                        seed=SEED, dtype=tf.float32))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64],\n",
    "                           dtype=tf.float32))\n",
    "# fully connected, depth 512.\n",
    "fc1_weights = tf.Variable(tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, \n",
    "                                               512],\n",
    "                        stddev=0.1,\n",
    "                        seed=SEED,\n",
    "                        dtype=tf.float32))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512],\n",
    "                         dtype=tf.float32))\n",
    "fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS],\n",
    "                                              stddev=0.1,\n",
    "                                              seed=SEED,\n",
    "                                              dtype=tf.float32))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS], \n",
    "                                     dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights)\n",
    "                + tf.nn.l2_loss(fc1_biases)\n",
    "                + tf.nn.l2_loss(fc2_weights)\n",
    "                + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0, dtype=tf.float32)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    0.01,                # Base learning rate.\n",
    "    batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "    train_size,          # Decay step.\n",
    "    0.95,                # Decay rate.\n",
    "    staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for the current training minibatch.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Predictions for the test and validation, which we'll compute less\n",
    "# often.\n",
    "eval_prediction = tf.nn.softmax(model(eval_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small utility function to evaluate a dataset by feeding batches of\n",
    "# data to {eval_data} and pulling the results from {eval_predictions}.\n",
    "# Saves memory and enables this to run on smaller GPUs.\n",
    "def eval_in_batches(data, sess):\n",
    "    \"\"\"Get predictions for a dataset by running it in small batches.\"\"\"\n",
    "    size = data.shape[0]\n",
    "    if size < EVAL_BATCH_SIZE:\n",
    "        raise ValueError(\"batch size for evals larger than dataset: %d\" \n",
    "                         % size)\n",
    "    predictions = np.ndarray(shape=(size, NUM_LABELS),\n",
    "                                dtype=np.float32)\n",
    "    for begin in xrange(0, size, EVAL_BATCH_SIZE):\n",
    "        end = begin + EVAL_BATCH_SIZE\n",
    "        if end <= size:\n",
    "            predictions[begin:end, :] = sess.run(\n",
    "                eval_prediction,\n",
    "                feed_dict={eval_data: data[begin:end, ...]})\n",
    "        else:\n",
    "            batch_predictions = sess.run(\n",
    "                eval_prediction,\n",
    "                feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
    "            predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (epoch 0.00), 20.3 ms\n",
      "Minibatch loss: 8.381, learning rate: 0.010000\n",
      "Minibatch error: 89.1%\n",
      "Validation error: 77.6%\n",
      "Step 100 (epoch 0.13), 2163.0 ms\n",
      "Minibatch loss: 3.309, learning rate: 0.010000\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 7.4%\n",
      "Step 200 (epoch 0.26), 2336.1 ms\n",
      "Minibatch loss: 3.267, learning rate: 0.010000\n",
      "Minibatch error: 7.8%\n",
      "Validation error: 7.4%\n",
      "Step 300 (epoch 0.38), 2322.9 ms\n",
      "Minibatch loss: 3.189, learning rate: 0.010000\n",
      "Minibatch error: 6.2%\n",
      "Validation error: 4.0%\n",
      "Step 400 (epoch 0.51), 2254.3 ms\n",
      "Minibatch loss: 3.243, learning rate: 0.010000\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 3.1%\n",
      "Step 500 (epoch 0.64), 2234.5 ms\n",
      "Minibatch loss: 3.122, learning rate: 0.010000\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 3.0%\n",
      "Step 600 (epoch 0.77), 2200.1 ms\n",
      "Minibatch loss: 2.950, learning rate: 0.010000\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.7%\n",
      "Step 700 (epoch 0.90), 2216.1 ms\n",
      "Minibatch loss: 2.985, learning rate: 0.010000\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 2.5%\n",
      "Step 800 (epoch 1.02), 2270.5 ms\n",
      "Minibatch loss: 2.925, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.3%\n",
      "Step 900 (epoch 1.15), 2299.3 ms\n",
      "Minibatch loss: 3.008, learning rate: 0.009500\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 2.6%\n",
      "Step 1000 (epoch 1.28), 2273.2 ms\n",
      "Minibatch loss: 2.861, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.2%\n",
      "Step 1100 (epoch 1.41), 2198.4 ms\n",
      "Minibatch loss: 2.858, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 2.4%\n",
      "Step 1200 (epoch 1.54), 2236.1 ms\n",
      "Minibatch loss: 2.842, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 2.2%\n",
      "Step 1300 (epoch 1.66), 2179.5 ms\n",
      "Minibatch loss: 2.753, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.2%\n",
      "Step 1400 (epoch 1.79), 2155.9 ms\n",
      "Minibatch loss: 2.755, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.0%\n",
      "Step 1500 (epoch 1.92), 2162.1 ms\n",
      "Minibatch loss: 2.745, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.8%\n",
      "Step 1600 (epoch 2.05), 2181.8 ms\n",
      "Minibatch loss: 2.760, learning rate: 0.009025\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.8%\n",
      "Step 1700 (epoch 2.18), 2286.4 ms\n",
      "Minibatch loss: 2.678, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.0%\n",
      "Step 1800 (epoch 2.30), 2201.1 ms\n",
      "Minibatch loss: 2.629, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 1900 (epoch 2.43), 2280.0 ms\n",
      "Minibatch loss: 2.669, learning rate: 0.009025\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.7%\n",
      "Step 2000 (epoch 2.56), 2293.6 ms\n",
      "Minibatch loss: 2.605, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 2100 (epoch 2.69), 2226.3 ms\n",
      "Minibatch loss: 2.602, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.8%\n",
      "Step 2200 (epoch 2.82), 2189.8 ms\n",
      "Minibatch loss: 2.550, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.5%\n",
      "Step 2300 (epoch 2.94), 2190.8 ms\n",
      "Minibatch loss: 2.553, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.7%\n",
      "Step 2400 (epoch 3.07), 2133.9 ms\n",
      "Minibatch loss: 2.575, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.7%\n",
      "Step 2500 (epoch 3.20), 2240.0 ms\n",
      "Minibatch loss: 2.509, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.6%\n",
      "Step 2600 (epoch 3.33), 2229.2 ms\n",
      "Minibatch loss: 2.658, learning rate: 0.008574\n",
      "Minibatch error: 6.2%\n",
      "Validation error: 1.5%\n",
      "Step 2700 (epoch 3.46), 2184.4 ms\n",
      "Minibatch loss: 2.434, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.5%\n",
      "Step 2800 (epoch 3.58), 2149.7 ms\n",
      "Minibatch loss: 2.559, learning rate: 0.008574\n",
      "Minibatch error: 6.2%\n",
      "Validation error: 1.4%\n",
      "Step 2900 (epoch 3.71), 3903.2 ms\n",
      "Minibatch loss: 2.441, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.7%\n",
      "Step 3000 (epoch 3.84), 4070.4 ms\n",
      "Minibatch loss: 2.473, learning rate: 0.008574\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.3%\n",
      "Step 3100 (epoch 3.97), 4169.0 ms\n",
      "Minibatch loss: 2.387, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.6%\n",
      "Step 3200 (epoch 4.10), 4124.1 ms\n",
      "Minibatch loss: 2.444, learning rate: 0.008145\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 1.3%\n",
      "Step 3300 (epoch 4.22), 2120.0 ms\n",
      "Minibatch loss: 2.312, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.5%\n",
      "Step 3400 (epoch 4.35), 2169.8 ms\n",
      "Minibatch loss: 2.298, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.4%\n",
      "Step 3500 (epoch 4.48), 2120.0 ms\n",
      "Minibatch loss: 2.283, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.4%\n",
      "Step 3600 (epoch 4.61), 2187.2 ms\n",
      "Minibatch loss: 2.282, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 3700 (epoch 4.74), 2131.1 ms\n",
      "Minibatch loss: 2.309, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.4%\n",
      "Step 3800 (epoch 4.86), 2081.7 ms\n",
      "Minibatch loss: 2.272, learning rate: 0.008145\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.3%\n",
      "Step 3900 (epoch 4.99), 2066.1 ms\n",
      "Minibatch loss: 2.546, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.3%\n",
      "Step 4000 (epoch 5.12), 2174.4 ms\n",
      "Minibatch loss: 2.209, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.4%\n",
      "Step 4100 (epoch 5.25), 2202.3 ms\n",
      "Minibatch loss: 2.186, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.3%\n",
      "Step 4200 (epoch 5.38), 2040.3 ms\n",
      "Minibatch loss: 2.219, learning rate: 0.007738\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.3%\n",
      "Step 4300 (epoch 5.50), 2041.8 ms\n",
      "Minibatch loss: 2.144, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 4400 (epoch 5.63), 2088.7 ms\n",
      "Minibatch loss: 2.153, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 4500 (epoch 5.76), 2051.9 ms\n",
      "Minibatch loss: 2.135, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 4600 (epoch 5.89), 2104.6 ms\n",
      "Minibatch loss: 2.107, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 4700 (epoch 6.02), 2088.0 ms\n",
      "Minibatch loss: 2.160, learning rate: 0.007351\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.5%\n",
      "Step 4800 (epoch 6.14), 2184.8 ms\n",
      "Minibatch loss: 2.077, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.3%\n",
      "Step 4900 (epoch 6.27), 2105.2 ms\n",
      "Minibatch loss: 2.041, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.3%\n",
      "Step 5000 (epoch 6.40), 2117.9 ms\n",
      "Minibatch loss: 2.029, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.3%\n",
      "Step 5100 (epoch 6.53), 2141.0 ms\n",
      "Minibatch loss: 2.015, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 5200 (epoch 6.66), 2194.6 ms\n",
      "Minibatch loss: 2.037, learning rate: 0.007351\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.1%\n",
      "Step 5300 (epoch 6.78), 2150.2 ms\n",
      "Minibatch loss: 2.004, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 5400 (epoch 6.91), 2123.9 ms\n",
      "Minibatch loss: 2.018, learning rate: 0.007351\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 5500 (epoch 7.04), 2179.6 ms\n",
      "Minibatch loss: 1.973, learning rate: 0.006983\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.3%\n",
      "Step 5600 (epoch 7.17), 2126.0 ms\n",
      "Minibatch loss: 1.938, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 5700 (epoch 7.30), 2272.9 ms\n",
      "Minibatch loss: 1.970, learning rate: 0.006983\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.1%\n",
      "Step 5800 (epoch 7.42), 2237.6 ms\n",
      "Minibatch loss: 1.920, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 5900 (epoch 7.55), 2322.0 ms\n",
      "Minibatch loss: 1.905, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 6000 (epoch 7.68), 2370.7 ms\n",
      "Minibatch loss: 1.928, learning rate: 0.006983\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.1%\n",
      "Step 6100 (epoch 7.81), 2240.9 ms\n",
      "Minibatch loss: 1.906, learning rate: 0.006983\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 6200 (epoch 7.94), 2176.2 ms\n",
      "Minibatch loss: 1.894, learning rate: 0.006983\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.2%\n",
      "Step 6300 (epoch 8.06), 2173.0 ms\n",
      "Minibatch loss: 1.863, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 6400 (epoch 8.19), 2120.2 ms\n",
      "Minibatch loss: 1.860, learning rate: 0.006634\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 6500 (epoch 8.32), 2103.7 ms\n",
      "Minibatch loss: 1.945, learning rate: 0.006634\n",
      "Minibatch error: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.2%\n",
      "Step 6600 (epoch 8.45), 2114.6 ms\n",
      "Minibatch loss: 1.811, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 6700 (epoch 8.58), 2089.6 ms\n",
      "Minibatch loss: 1.840, learning rate: 0.006634\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 6800 (epoch 8.70), 2104.4 ms\n",
      "Minibatch loss: 1.790, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.3%\n",
      "Step 6900 (epoch 8.83), 2201.0 ms\n",
      "Minibatch loss: 1.793, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 7000 (epoch 8.96), 2172.8 ms\n",
      "Minibatch loss: 1.778, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 7100 (epoch 9.09), 2096.9 ms\n",
      "Minibatch loss: 1.753, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 7200 (epoch 9.22), 2315.8 ms\n",
      "Minibatch loss: 1.743, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 7300 (epoch 9.34), 2409.9 ms\n",
      "Minibatch loss: 1.749, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 7400 (epoch 9.47), 2244.4 ms\n",
      "Minibatch loss: 1.720, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 7500 (epoch 9.60), 2160.7 ms\n",
      "Minibatch loss: 1.719, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 7600 (epoch 9.73), 2163.5 ms\n",
      "Minibatch loss: 1.751, learning rate: 0.006302\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 7700 (epoch 9.86), 2162.6 ms\n",
      "Minibatch loss: 1.773, learning rate: 0.006302\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.1%\n",
      "Step 7800 (epoch 9.98), 2166.5 ms\n",
      "Minibatch loss: 1.678, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Test error: 1.0%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "from six.moves import xrange\n",
    "\n",
    "# Create a local session to run the training.\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    # Run all the initializers to prepare the trainable parameters.\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Loop through training steps.\n",
    "    for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n",
    "        # Compute the offset of the current minibatch in the data.\n",
    "        # Note that we could use better randomization across epochs.\n",
    "        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "        batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "        # This dictionary maps the batch data (as a numpy array) to the\n",
    "        # node in the graph it should be fed to.\n",
    "        feed_dict = {train_data_node: batch_data,\n",
    "                     train_labels_node: batch_labels}\n",
    "        # Run the optimizer to update weights.\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        # print some extra information once reach the evaluation frequency\n",
    "        if step % EVAL_FREQUENCY == 0:\n",
    "            # fetch some extra nodes' data\n",
    "            l, lr, predictions = sess.run([loss, learning_rate,\n",
    "                                           train_prediction],\n",
    "                                          feed_dict=feed_dict)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            start_time = time.time()\n",
    "            print('Step %d (epoch %.2f), %.1f ms' %\n",
    "                  (step, float(step) * BATCH_SIZE / train_size,\n",
    "                   1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "            print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "            print('Minibatch error: %.1f%%'\n",
    "                % error_rate(predictions, batch_labels))\n",
    "            print('Validation error: %.1f%%' % error_rate(\n",
    "                eval_in_batches(validation_data, sess), validation_labels))\n",
    "            sys.stdout.flush()\n",
    "    # Finally print the result!\n",
    "    test_error = error_rate(eval_in_batches(test_data, sess),\n",
    "                            test_labels)\n",
    "    print('Test error: %.1f%%' % test_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
