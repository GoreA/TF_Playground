{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Model hyperparameters\n",
      "n_hidden = 50\n",
      "n_layers = 1\n",
      "learning_rate = 0.001000\n",
      "n_epochs = 45\n",
      "batch_size = 1000\n",
      "weight_positives = True\n",
      "dropout_prob = 0.500000\n",
      "---------------------------------------------\n",
      "epoch 0, step 0, loss: 87202400.000000\n",
      "epoch 0, step 5, loss: 76459776.000000\n",
      "epoch 0, step 10, loss: 54304912.000000\n",
      "epoch 0, step 15, loss: 45205516.000000\n",
      "epoch 0, step 20, loss: 49988572.000000\n",
      "epoch 0, step 25, loss: 42179988.000000\n",
      "epoch 0, step 30, loss: 45471116.000000\n",
      "epoch 0, step 35, loss: 37261576.000000\n",
      "epoch 0, step 40, loss: 33394884.000000\n",
      "epoch 0, step 45, loss: 31296276.000000\n",
      "epoch 0, step 50, loss: 30412180.000000\n",
      "epoch 0, step 55, loss: 29236714.000000\n",
      "epoch 1, step 60, loss: 27342348.000000\n",
      "epoch 1, step 65, loss: 27847810.000000\n",
      "epoch 1, step 70, loss: 24453852.000000\n",
      "epoch 1, step 75, loss: 20952502.000000\n",
      "epoch 1, step 80, loss: 27360068.000000\n",
      "epoch 1, step 85, loss: 19555060.000000\n",
      "epoch 1, step 90, loss: 23736836.000000\n",
      "epoch 1, step 95, loss: 18291132.000000\n",
      "epoch 1, step 100, loss: 18172872.000000\n",
      "epoch 1, step 105, loss: 17515396.000000\n",
      "epoch 1, step 110, loss: 19291468.000000\n",
      "epoch 1, step 115, loss: 17022224.000000\n",
      "epoch 2, step 120, loss: 14972352.000000\n",
      "epoch 2, step 125, loss: 15675698.000000\n",
      "epoch 2, step 130, loss: 14870486.000000\n",
      "epoch 2, step 135, loss: 11719498.000000\n",
      "epoch 2, step 140, loss: 15200485.000000\n",
      "epoch 2, step 145, loss: 12872282.000000\n",
      "epoch 2, step 150, loss: 12974463.000000\n",
      "epoch 2, step 155, loss: 9959884.000000\n",
      "epoch 2, step 160, loss: 10863022.000000\n",
      "epoch 2, step 165, loss: 11087342.000000\n",
      "epoch 2, step 170, loss: 11453829.000000\n",
      "epoch 2, step 175, loss: 10586668.000000\n",
      "epoch 3, step 180, loss: 8466184.000000\n",
      "epoch 3, step 185, loss: 8780130.000000\n",
      "epoch 3, step 190, loss: 8471885.000000\n",
      "epoch 3, step 195, loss: 7335256.000000\n",
      "epoch 3, step 200, loss: 9496529.000000\n",
      "epoch 3, step 205, loss: 7258462.000000\n",
      "epoch 3, step 210, loss: 8154810.500000\n",
      "epoch 3, step 215, loss: 7313769.000000\n",
      "epoch 3, step 220, loss: 7361777.500000\n",
      "epoch 3, step 225, loss: 6833115.000000\n",
      "epoch 3, step 230, loss: 5920427.000000\n",
      "epoch 3, step 235, loss: 5786079.500000\n",
      "epoch 4, step 240, loss: 6650248.500000\n",
      "epoch 4, step 245, loss: 5882170.500000\n",
      "epoch 4, step 250, loss: 6185087.000000\n",
      "epoch 4, step 255, loss: 4895353.000000\n",
      "epoch 4, step 260, loss: 6892498.000000\n",
      "epoch 4, step 265, loss: 5113046.000000\n",
      "epoch 4, step 270, loss: 5406513.500000\n",
      "epoch 4, step 275, loss: 4637290.500000\n",
      "epoch 4, step 280, loss: 4803952.500000\n",
      "epoch 4, step 285, loss: 4366847.000000\n",
      "epoch 4, step 290, loss: 4713147.000000\n",
      "epoch 4, step 295, loss: 4209015.000000\n",
      "epoch 5, step 300, loss: 4528756.500000\n",
      "epoch 5, step 305, loss: 4379892.000000\n",
      "epoch 5, step 310, loss: 3312477.000000\n",
      "epoch 5, step 315, loss: 4006845.000000\n",
      "epoch 5, step 320, loss: 3923203.000000\n",
      "epoch 5, step 325, loss: 3853769.500000\n",
      "epoch 5, step 330, loss: 4300538.000000\n",
      "epoch 5, step 335, loss: 3164903.750000\n",
      "epoch 5, step 340, loss: 3613028.250000\n",
      "epoch 5, step 345, loss: 3404124.750000\n",
      "epoch 5, step 350, loss: 2976100.500000\n",
      "epoch 5, step 355, loss: 3555314.750000\n",
      "epoch 6, step 360, loss: 3525992.000000\n",
      "epoch 6, step 365, loss: 2814124.000000\n",
      "epoch 6, step 370, loss: 2851312.500000\n",
      "epoch 6, step 375, loss: 2834284.500000\n",
      "epoch 6, step 380, loss: 3449024.750000\n",
      "epoch 6, step 385, loss: 2924681.000000\n",
      "epoch 6, step 390, loss: 3002918.000000\n",
      "epoch 6, step 395, loss: 2657542.500000\n",
      "epoch 6, step 400, loss: 2734902.500000\n",
      "epoch 6, step 405, loss: 2687590.000000\n",
      "epoch 6, step 410, loss: 2462153.000000\n",
      "epoch 6, step 415, loss: 2687272.500000\n",
      "epoch 7, step 420, loss: 2722569.750000\n",
      "epoch 7, step 425, loss: 2276755.000000\n",
      "epoch 7, step 430, loss: 1928213.375000\n",
      "epoch 7, step 435, loss: 2049196.500000\n",
      "epoch 7, step 440, loss: 2508678.000000\n",
      "epoch 7, step 445, loss: 2150006.500000\n",
      "epoch 7, step 450, loss: 2415999.500000\n",
      "epoch 7, step 455, loss: 1917386.750000\n",
      "epoch 7, step 460, loss: 2150258.500000\n",
      "epoch 7, step 465, loss: 2425233.500000\n",
      "epoch 7, step 470, loss: 2125743.000000\n",
      "epoch 7, step 475, loss: 2095778.750000\n",
      "epoch 8, step 480, loss: 2047480.125000\n",
      "epoch 8, step 485, loss: 1716420.750000\n",
      "epoch 8, step 490, loss: 2012491.000000\n",
      "epoch 8, step 495, loss: 2030267.750000\n",
      "epoch 8, step 500, loss: 1922908.250000\n",
      "epoch 8, step 505, loss: 1696786.000000\n",
      "epoch 8, step 510, loss: 2227239.000000\n",
      "epoch 8, step 515, loss: 1815805.000000\n",
      "epoch 8, step 520, loss: 1742527.750000\n",
      "epoch 8, step 525, loss: 1606150.375000\n",
      "epoch 8, step 530, loss: 1606280.625000\n",
      "epoch 8, step 535, loss: 1395988.000000\n",
      "epoch 9, step 540, loss: 1761551.000000\n",
      "epoch 9, step 545, loss: 1394471.875000\n",
      "epoch 9, step 550, loss: 1178190.000000\n",
      "epoch 9, step 555, loss: 1316137.500000\n",
      "epoch 9, step 560, loss: 1731307.750000\n",
      "epoch 9, step 565, loss: 1400728.500000\n",
      "epoch 9, step 570, loss: 1667667.500000\n",
      "epoch 9, step 575, loss: 1104538.750000\n",
      "epoch 9, step 580, loss: 1261752.375000\n",
      "epoch 9, step 585, loss: 1367060.250000\n",
      "epoch 9, step 590, loss: 1327650.875000\n",
      "epoch 9, step 595, loss: 1194846.500000\n",
      "epoch 10, step 600, loss: 1413667.125000\n",
      "epoch 10, step 605, loss: 1128143.750000\n",
      "epoch 10, step 610, loss: 1088563.750000\n",
      "epoch 10, step 615, loss: 1124843.500000\n",
      "epoch 10, step 620, loss: 1449230.125000\n",
      "epoch 10, step 625, loss: 1109389.125000\n",
      "epoch 10, step 630, loss: 1281628.625000\n",
      "epoch 10, step 635, loss: 1152594.750000\n",
      "epoch 10, step 640, loss: 1116080.750000\n",
      "epoch 10, step 645, loss: 1118461.125000\n",
      "epoch 10, step 650, loss: 1152777.000000\n",
      "epoch 10, step 655, loss: 1237501.750000\n",
      "epoch 11, step 660, loss: 1279617.125000\n",
      "epoch 11, step 665, loss: 892525.312500\n",
      "epoch 11, step 670, loss: 976743.625000\n",
      "epoch 11, step 675, loss: 1119719.000000\n",
      "epoch 11, step 680, loss: 1004915.312500\n",
      "epoch 11, step 685, loss: 931545.500000\n",
      "epoch 11, step 690, loss: 1282412.250000\n",
      "epoch 11, step 695, loss: 878078.125000\n",
      "epoch 11, step 700, loss: 856774.125000\n",
      "epoch 11, step 705, loss: 855949.187500\n",
      "epoch 11, step 710, loss: 901152.000000\n",
      "epoch 11, step 715, loss: 737457.125000\n",
      "epoch 12, step 720, loss: 968585.250000\n",
      "epoch 12, step 725, loss: 749133.375000\n",
      "epoch 12, step 730, loss: 793266.250000\n",
      "epoch 12, step 735, loss: 844052.375000\n",
      "epoch 12, step 740, loss: 855266.375000\n",
      "epoch 12, step 745, loss: 797804.437500\n",
      "epoch 12, step 750, loss: 866467.125000\n",
      "epoch 12, step 755, loss: 689550.375000\n",
      "epoch 12, step 760, loss: 847466.062500\n",
      "epoch 12, step 765, loss: 824584.562500\n",
      "epoch 12, step 770, loss: 723079.062500\n",
      "epoch 12, step 775, loss: 757142.000000\n",
      "epoch 13, step 780, loss: 862558.937500\n",
      "epoch 13, step 785, loss: 675238.875000\n",
      "epoch 13, step 790, loss: 607443.687500\n",
      "epoch 13, step 795, loss: 772788.750000\n",
      "epoch 13, step 800, loss: 725527.937500\n",
      "epoch 13, step 805, loss: 796158.312500\n",
      "epoch 13, step 810, loss: 998180.125000\n",
      "epoch 13, step 815, loss: 581922.250000\n",
      "epoch 13, step 820, loss: 726236.500000\n",
      "epoch 13, step 825, loss: 670092.750000\n",
      "epoch 13, step 830, loss: 604211.625000\n",
      "epoch 13, step 835, loss: 650845.687500\n",
      "epoch 14, step 840, loss: 703141.562500\n",
      "epoch 14, step 845, loss: 555060.625000\n",
      "epoch 14, step 850, loss: 601190.500000\n",
      "epoch 14, step 855, loss: 643846.250000\n",
      "epoch 14, step 860, loss: 593191.812500\n",
      "epoch 14, step 865, loss: 650336.375000\n",
      "epoch 14, step 870, loss: 829434.000000\n",
      "epoch 14, step 875, loss: 519293.625000\n",
      "epoch 14, step 880, loss: 536802.375000\n",
      "epoch 14, step 885, loss: 678963.000000\n",
      "epoch 14, step 890, loss: 550571.875000\n",
      "epoch 14, step 895, loss: 583591.250000\n",
      "epoch 15, step 900, loss: 663088.250000\n",
      "epoch 15, step 905, loss: 526606.875000\n",
      "epoch 15, step 910, loss: 400014.500000\n",
      "epoch 15, step 915, loss: 603014.875000\n",
      "epoch 15, step 920, loss: 601711.750000\n",
      "epoch 15, step 925, loss: 562528.250000\n",
      "epoch 15, step 930, loss: 728869.625000\n",
      "epoch 15, step 935, loss: 483020.625000\n",
      "epoch 15, step 940, loss: 503832.125000\n",
      "epoch 15, step 945, loss: 616341.250000\n",
      "epoch 15, step 950, loss: 528665.250000\n",
      "epoch 15, step 955, loss: 481970.625000\n",
      "epoch 16, step 960, loss: 539692.500000\n",
      "epoch 16, step 965, loss: 454670.000000\n",
      "epoch 16, step 970, loss: 454388.187500\n",
      "epoch 16, step 975, loss: 510971.625000\n",
      "epoch 16, step 980, loss: 586721.812500\n",
      "epoch 16, step 985, loss: 497297.656250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, step 990, loss: 522920.125000\n",
      "epoch 16, step 995, loss: 524085.000000\n",
      "epoch 16, step 1000, loss: 477812.062500\n",
      "epoch 16, step 1005, loss: 577083.937500\n",
      "epoch 16, step 1010, loss: 376508.250000\n",
      "epoch 16, step 1015, loss: 441041.406250\n",
      "epoch 17, step 1020, loss: 597320.875000\n",
      "epoch 17, step 1025, loss: 354206.437500\n",
      "epoch 17, step 1030, loss: 399911.500000\n",
      "epoch 17, step 1035, loss: 484546.812500\n",
      "epoch 17, step 1040, loss: 508130.468750\n",
      "epoch 17, step 1045, loss: 443270.968750\n",
      "epoch 17, step 1050, loss: 539542.187500\n",
      "epoch 17, step 1055, loss: 351285.250000\n",
      "epoch 17, step 1060, loss: 428697.312500\n",
      "epoch 17, step 1065, loss: 372741.718750\n",
      "epoch 17, step 1070, loss: 415228.312500\n",
      "epoch 17, step 1075, loss: 329338.875000\n",
      "epoch 18, step 1080, loss: 481071.312500\n",
      "epoch 18, step 1085, loss: 371011.875000\n",
      "epoch 18, step 1090, loss: 320539.187500\n",
      "epoch 18, step 1095, loss: 384345.156250\n",
      "epoch 18, step 1100, loss: 374835.125000\n",
      "epoch 18, step 1105, loss: 448564.656250\n",
      "epoch 18, step 1110, loss: 460964.000000\n",
      "epoch 18, step 1115, loss: 325822.000000\n",
      "epoch 18, step 1120, loss: 336013.187500\n",
      "epoch 18, step 1125, loss: 392591.062500\n",
      "epoch 18, step 1130, loss: 401894.562500\n",
      "epoch 18, step 1135, loss: 385705.656250\n",
      "epoch 19, step 1140, loss: 394692.500000\n",
      "epoch 19, step 1145, loss: 358870.187500\n",
      "epoch 19, step 1150, loss: 327869.875000\n",
      "epoch 19, step 1155, loss: 374749.906250\n",
      "epoch 19, step 1160, loss: 387769.312500\n",
      "epoch 19, step 1165, loss: 380813.937500\n",
      "epoch 19, step 1170, loss: 486729.125000\n",
      "epoch 19, step 1175, loss: 281892.750000\n",
      "epoch 19, step 1180, loss: 315213.968750\n",
      "epoch 19, step 1185, loss: 352805.625000\n",
      "epoch 19, step 1190, loss: 327702.375000\n",
      "epoch 19, step 1195, loss: 327300.218750\n",
      "epoch 20, step 1200, loss: 408194.062500\n",
      "epoch 20, step 1205, loss: 261626.312500\n",
      "epoch 20, step 1210, loss: 249179.343750\n",
      "epoch 20, step 1215, loss: 343149.468750\n",
      "epoch 20, step 1220, loss: 273975.062500\n",
      "epoch 20, step 1225, loss: 335527.718750\n",
      "epoch 20, step 1230, loss: 338664.562500\n",
      "epoch 20, step 1235, loss: 254061.468750\n",
      "epoch 20, step 1240, loss: 336064.500000\n",
      "epoch 20, step 1245, loss: 326544.000000\n",
      "epoch 20, step 1250, loss: 293454.937500\n",
      "epoch 20, step 1255, loss: 275105.125000\n",
      "epoch 21, step 1260, loss: 408649.250000\n",
      "epoch 21, step 1265, loss: 236468.984375\n",
      "epoch 21, step 1270, loss: 274996.500000\n",
      "epoch 21, step 1275, loss: 341526.312500\n",
      "epoch 21, step 1280, loss: 354375.812500\n",
      "epoch 21, step 1285, loss: 272533.062500\n",
      "epoch 21, step 1290, loss: 378850.187500\n",
      "epoch 21, step 1295, loss: 217458.656250\n",
      "epoch 21, step 1300, loss: 300300.250000\n",
      "epoch 21, step 1305, loss: 284773.687500\n",
      "epoch 21, step 1310, loss: 235264.718750\n",
      "epoch 21, step 1315, loss: 277577.093750\n",
      "epoch 22, step 1320, loss: 270477.375000\n",
      "epoch 22, step 1325, loss: 187659.062500\n",
      "epoch 22, step 1330, loss: 239355.015625\n",
      "epoch 22, step 1335, loss: 284858.531250\n",
      "epoch 22, step 1340, loss: 288600.812500\n",
      "epoch 22, step 1345, loss: 247337.062500\n",
      "epoch 22, step 1350, loss: 315803.375000\n",
      "epoch 22, step 1355, loss: 224418.484375\n",
      "epoch 22, step 1360, loss: 288575.125000\n",
      "epoch 22, step 1365, loss: 252940.218750\n",
      "epoch 22, step 1370, loss: 200473.000000\n",
      "epoch 22, step 1375, loss: 229497.843750\n",
      "epoch 23, step 1380, loss: 275304.312500\n",
      "epoch 23, step 1385, loss: 199877.531250\n",
      "epoch 23, step 1390, loss: 222067.750000\n",
      "epoch 23, step 1395, loss: 290345.531250\n",
      "epoch 23, step 1400, loss: 218038.937500\n",
      "epoch 23, step 1405, loss: 250041.812500\n",
      "epoch 23, step 1410, loss: 306013.562500\n",
      "epoch 23, step 1415, loss: 184850.031250\n",
      "epoch 23, step 1420, loss: 209957.656250\n",
      "epoch 23, step 1425, loss: 250213.687500\n",
      "epoch 23, step 1430, loss: 228977.484375\n",
      "epoch 23, step 1435, loss: 233530.437500\n",
      "epoch 24, step 1440, loss: 271713.531250\n",
      "epoch 24, step 1445, loss: 189603.953125\n",
      "epoch 24, step 1450, loss: 200702.078125\n",
      "epoch 24, step 1455, loss: 255737.765625\n",
      "epoch 24, step 1460, loss: 218651.640625\n",
      "epoch 24, step 1465, loss: 193434.781250\n",
      "epoch 24, step 1470, loss: 285373.187500\n",
      "epoch 24, step 1475, loss: 163511.921875\n",
      "epoch 24, step 1480, loss: 194030.656250\n",
      "epoch 24, step 1485, loss: 197394.500000\n",
      "epoch 24, step 1490, loss: 188921.171875\n",
      "epoch 24, step 1495, loss: 209039.765625\n",
      "epoch 25, step 1500, loss: 237975.890625\n",
      "epoch 25, step 1505, loss: 207454.156250\n",
      "epoch 25, step 1510, loss: 157605.062500\n",
      "epoch 25, step 1515, loss: 241229.500000\n",
      "epoch 25, step 1520, loss: 209176.250000\n",
      "epoch 25, step 1525, loss: 219044.234375\n",
      "epoch 25, step 1530, loss: 312524.312500\n",
      "epoch 25, step 1535, loss: 161875.531250\n",
      "epoch 25, step 1540, loss: 231681.562500\n",
      "epoch 25, step 1545, loss: 206295.453125\n",
      "epoch 25, step 1550, loss: 158350.718750\n",
      "epoch 25, step 1555, loss: 199527.343750\n",
      "epoch 26, step 1560, loss: 235218.265625\n",
      "epoch 26, step 1565, loss: 167443.906250\n",
      "epoch 26, step 1570, loss: 130447.828125\n",
      "epoch 26, step 1575, loss: 194870.421875\n",
      "epoch 26, step 1580, loss: 187386.984375\n",
      "epoch 26, step 1585, loss: 178397.375000\n",
      "epoch 26, step 1590, loss: 283967.375000\n",
      "epoch 26, step 1595, loss: 163474.375000\n",
      "epoch 26, step 1600, loss: 197739.281250\n",
      "epoch 26, step 1605, loss: 203455.093750\n",
      "epoch 26, step 1610, loss: 165480.593750\n",
      "epoch 26, step 1615, loss: 155593.875000\n",
      "epoch 27, step 1620, loss: 183006.250000\n",
      "epoch 27, step 1625, loss: 150810.828125\n",
      "epoch 27, step 1630, loss: 150356.093750\n",
      "epoch 27, step 1635, loss: 212902.437500\n",
      "epoch 27, step 1640, loss: 167856.578125\n",
      "epoch 27, step 1645, loss: 178685.343750\n",
      "epoch 27, step 1650, loss: 230883.328125\n",
      "epoch 27, step 1655, loss: 145310.015625\n",
      "epoch 27, step 1660, loss: 154143.812500\n",
      "epoch 27, step 1665, loss: 183485.906250\n",
      "epoch 27, step 1670, loss: 136347.640625\n",
      "epoch 27, step 1675, loss: 162112.281250\n",
      "epoch 28, step 1680, loss: 216619.656250\n",
      "epoch 28, step 1685, loss: 158044.437500\n",
      "epoch 28, step 1690, loss: 150198.687500\n",
      "epoch 28, step 1695, loss: 192292.093750\n",
      "epoch 28, step 1700, loss: 146475.312500\n",
      "epoch 28, step 1705, loss: 161130.500000\n",
      "epoch 28, step 1710, loss: 288721.500000\n",
      "epoch 28, step 1715, loss: 165915.500000\n",
      "epoch 28, step 1720, loss: 195246.906250\n",
      "epoch 28, step 1725, loss: 153195.390625\n",
      "epoch 28, step 1730, loss: 146922.000000\n",
      "epoch 28, step 1735, loss: 118697.179688\n",
      "epoch 29, step 1740, loss: 155532.968750\n",
      "epoch 29, step 1745, loss: 124190.054688\n",
      "epoch 29, step 1750, loss: 117488.000000\n",
      "epoch 29, step 1755, loss: 189456.312500\n",
      "epoch 29, step 1760, loss: 187213.593750\n",
      "epoch 29, step 1765, loss: 164587.656250\n",
      "epoch 29, step 1770, loss: 206349.109375\n",
      "epoch 29, step 1775, loss: 121246.562500\n",
      "epoch 29, step 1780, loss: 171407.890625\n",
      "epoch 29, step 1785, loss: 145516.125000\n",
      "epoch 29, step 1790, loss: 116427.445312\n",
      "epoch 29, step 1795, loss: 122118.125000\n",
      "epoch 30, step 1800, loss: 160725.015625\n",
      "epoch 30, step 1805, loss: 100596.187500\n",
      "epoch 30, step 1810, loss: 112852.609375\n",
      "epoch 30, step 1815, loss: 149613.750000\n",
      "epoch 30, step 1820, loss: 150190.812500\n",
      "epoch 30, step 1825, loss: 134423.203125\n",
      "epoch 30, step 1830, loss: 253311.546875\n",
      "epoch 30, step 1835, loss: 113203.117188\n",
      "epoch 30, step 1840, loss: 116316.414062\n",
      "epoch 30, step 1845, loss: 147102.812500\n",
      "epoch 30, step 1850, loss: 106510.796875\n",
      "epoch 30, step 1855, loss: 131768.875000\n",
      "epoch 31, step 1860, loss: 168164.875000\n",
      "epoch 31, step 1865, loss: 103672.265625\n",
      "epoch 31, step 1870, loss: 84636.000000\n",
      "epoch 31, step 1875, loss: 147853.140625\n",
      "epoch 31, step 1880, loss: 143471.359375\n",
      "epoch 31, step 1885, loss: 135833.000000\n",
      "epoch 31, step 1890, loss: 203033.562500\n",
      "epoch 31, step 1895, loss: 153007.468750\n",
      "epoch 31, step 1900, loss: 113820.187500\n",
      "epoch 31, step 1905, loss: 143082.187500\n",
      "epoch 31, step 1910, loss: 99331.015625\n",
      "epoch 31, step 1915, loss: 88444.250000\n",
      "epoch 32, step 1920, loss: 154916.343750\n",
      "epoch 32, step 1925, loss: 102535.007812\n",
      "epoch 32, step 1930, loss: 124242.703125\n",
      "epoch 32, step 1935, loss: 150323.453125\n",
      "epoch 32, step 1940, loss: 117450.734375\n",
      "epoch 32, step 1945, loss: 146382.156250\n",
      "epoch 32, step 1950, loss: 159180.531250\n",
      "epoch 32, step 1955, loss: 108097.484375\n",
      "epoch 32, step 1960, loss: 132027.125000\n",
      "epoch 32, step 1965, loss: 95706.796875\n",
      "epoch 32, step 1970, loss: 96703.312500\n",
      "epoch 32, step 1975, loss: 91018.062500\n",
      "epoch 33, step 1980, loss: 108149.765625\n",
      "epoch 33, step 1985, loss: 96936.406250\n",
      "epoch 33, step 1990, loss: 123694.609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33, step 1995, loss: 139885.875000\n",
      "epoch 33, step 2000, loss: 119088.546875\n",
      "epoch 33, step 2005, loss: 102525.867188\n",
      "epoch 33, step 2010, loss: 188728.296875\n",
      "epoch 33, step 2015, loss: 99522.484375\n",
      "epoch 33, step 2020, loss: 121766.140625\n",
      "epoch 33, step 2025, loss: 133892.531250\n",
      "epoch 33, step 2030, loss: 105343.734375\n",
      "epoch 33, step 2035, loss: 82786.250000\n",
      "epoch 34, step 2040, loss: 128918.390625\n",
      "epoch 34, step 2045, loss: 90317.281250\n",
      "epoch 34, step 2050, loss: 98674.640625\n",
      "epoch 34, step 2055, loss: 107518.117188\n",
      "epoch 34, step 2060, loss: 118727.187500\n",
      "epoch 34, step 2065, loss: 90695.500000\n",
      "epoch 34, step 2070, loss: 203405.406250\n",
      "epoch 34, step 2075, loss: 97660.296875\n",
      "epoch 34, step 2080, loss: 85687.343750\n",
      "epoch 34, step 2085, loss: 106600.476562\n",
      "epoch 34, step 2090, loss: 76029.000000\n",
      "epoch 34, step 2095, loss: 82386.953125\n",
      "epoch 35, step 2100, loss: 106133.546875\n",
      "epoch 35, step 2105, loss: 83232.156250\n",
      "epoch 35, step 2110, loss: 70689.953125\n",
      "epoch 35, step 2115, loss: 102666.007812\n",
      "epoch 35, step 2120, loss: 106659.953125\n",
      "epoch 35, step 2125, loss: 121240.531250\n",
      "epoch 35, step 2130, loss: 150672.437500\n",
      "epoch 35, step 2135, loss: 83122.960938\n",
      "epoch 35, step 2140, loss: 97779.492188\n",
      "epoch 35, step 2145, loss: 93132.828125\n",
      "epoch 35, step 2150, loss: 93703.273438\n",
      "epoch 35, step 2155, loss: 86024.234375\n",
      "epoch 36, step 2160, loss: 125128.664062\n",
      "epoch 36, step 2165, loss: 59904.574219\n",
      "epoch 36, step 2170, loss: 80499.406250\n",
      "epoch 36, step 2175, loss: 109951.796875\n",
      "epoch 36, step 2180, loss: 107177.484375\n",
      "epoch 36, step 2185, loss: 94462.000000\n",
      "epoch 36, step 2190, loss: 107916.265625\n",
      "epoch 36, step 2195, loss: 90636.968750\n",
      "epoch 36, step 2200, loss: 76656.156250\n",
      "epoch 36, step 2205, loss: 97897.882812\n",
      "epoch 36, step 2210, loss: 69619.570312\n",
      "epoch 36, step 2215, loss: 96198.109375\n",
      "epoch 37, step 2220, loss: 102642.671875\n",
      "epoch 37, step 2225, loss: 74880.859375\n",
      "epoch 37, step 2230, loss: 91884.578125\n",
      "epoch 37, step 2235, loss: 95165.281250\n",
      "epoch 37, step 2240, loss: 110629.335938\n",
      "epoch 37, step 2245, loss: 83013.890625\n",
      "epoch 37, step 2250, loss: 147914.500000\n",
      "epoch 37, step 2255, loss: 70394.898438\n",
      "epoch 37, step 2260, loss: 111954.085938\n",
      "epoch 37, step 2265, loss: 88117.921875\n",
      "epoch 37, step 2270, loss: 83267.843750\n",
      "epoch 37, step 2275, loss: 84049.593750\n",
      "epoch 38, step 2280, loss: 108311.500000\n",
      "epoch 38, step 2285, loss: 87983.562500\n",
      "epoch 38, step 2290, loss: 84563.445312\n",
      "epoch 38, step 2295, loss: 81494.515625\n",
      "epoch 38, step 2300, loss: 96497.578125\n",
      "epoch 38, step 2305, loss: 91186.906250\n",
      "epoch 38, step 2310, loss: 171633.687500\n",
      "epoch 38, step 2315, loss: 80204.546875\n",
      "epoch 38, step 2320, loss: 69738.437500\n",
      "epoch 38, step 2325, loss: 79632.953125\n",
      "epoch 38, step 2330, loss: 63650.441406\n",
      "epoch 38, step 2335, loss: 69429.203125\n",
      "epoch 39, step 2340, loss: 103042.398438\n",
      "epoch 39, step 2345, loss: 66836.703125\n",
      "epoch 39, step 2350, loss: 63946.835938\n",
      "epoch 39, step 2355, loss: 85387.406250\n",
      "epoch 39, step 2360, loss: 102301.843750\n",
      "epoch 39, step 2365, loss: 80666.109375\n",
      "epoch 39, step 2370, loss: 90723.125000\n",
      "epoch 39, step 2375, loss: 95967.703125\n",
      "epoch 39, step 2380, loss: 71771.125000\n",
      "epoch 39, step 2385, loss: 80650.046875\n",
      "epoch 39, step 2390, loss: 61618.695312\n",
      "epoch 39, step 2395, loss: 53846.753906\n",
      "epoch 40, step 2400, loss: 94406.078125\n",
      "epoch 40, step 2405, loss: 66444.898438\n",
      "epoch 40, step 2410, loss: 78494.656250\n",
      "epoch 40, step 2415, loss: 85367.046875\n",
      "epoch 40, step 2420, loss: 82039.828125\n",
      "epoch 40, step 2425, loss: 67466.921875\n",
      "epoch 40, step 2430, loss: 98235.906250\n",
      "epoch 40, step 2435, loss: 71649.984375\n",
      "epoch 40, step 2440, loss: 79337.906250\n",
      "epoch 40, step 2445, loss: 74788.484375\n",
      "epoch 40, step 2450, loss: 59830.679688\n",
      "epoch 40, step 2455, loss: 64918.468750\n",
      "epoch 41, step 2460, loss: 86978.578125\n",
      "epoch 41, step 2465, loss: 61218.328125\n",
      "epoch 41, step 2470, loss: 48283.851562\n",
      "epoch 41, step 2475, loss: 63323.808594\n",
      "epoch 41, step 2480, loss: 84665.703125\n",
      "epoch 41, step 2485, loss: 65040.351562\n",
      "epoch 41, step 2490, loss: 88494.695312\n",
      "epoch 41, step 2495, loss: 52492.078125\n",
      "epoch 41, step 2500, loss: 82417.906250\n",
      "epoch 41, step 2505, loss: 66155.171875\n",
      "epoch 41, step 2510, loss: 57172.140625\n",
      "epoch 41, step 2515, loss: 46859.472656\n",
      "epoch 42, step 2520, loss: 92829.953125\n",
      "epoch 42, step 2525, loss: 69507.109375\n",
      "epoch 42, step 2530, loss: 49637.898438\n",
      "epoch 42, step 2535, loss: 74362.625000\n",
      "epoch 42, step 2540, loss: 65083.656250\n",
      "epoch 42, step 2545, loss: 72482.531250\n",
      "epoch 42, step 2550, loss: 83834.687500\n",
      "epoch 42, step 2555, loss: 50185.437500\n",
      "epoch 42, step 2560, loss: 63727.093750\n",
      "epoch 42, step 2565, loss: 73895.015625\n",
      "epoch 42, step 2570, loss: 52505.253906\n",
      "epoch 42, step 2575, loss: 48307.484375\n",
      "epoch 43, step 2580, loss: 83377.148438\n",
      "epoch 43, step 2585, loss: 71822.187500\n",
      "epoch 43, step 2590, loss: 56067.578125\n",
      "epoch 43, step 2595, loss: 77665.632812\n",
      "epoch 43, step 2600, loss: 76775.976562\n",
      "epoch 43, step 2605, loss: 66824.015625\n",
      "epoch 43, step 2610, loss: 70074.968750\n",
      "epoch 43, step 2615, loss: 44934.292969\n",
      "epoch 43, step 2620, loss: 55513.265625\n",
      "epoch 43, step 2625, loss: 64376.226562\n",
      "epoch 43, step 2630, loss: 47518.660156\n",
      "epoch 43, step 2635, loss: 56056.816406\n",
      "epoch 44, step 2640, loss: 50650.132812\n",
      "epoch 44, step 2645, loss: 50847.234375\n",
      "epoch 44, step 2650, loss: 41615.613281\n",
      "epoch 44, step 2655, loss: 77872.484375\n",
      "epoch 44, step 2660, loss: 66030.437500\n",
      "epoch 44, step 2665, loss: 78059.750000\n",
      "epoch 44, step 2670, loss: 83576.453125\n",
      "epoch 44, step 2675, loss: 51513.640625\n",
      "epoch 44, step 2680, loss: 52456.578125\n",
      "epoch 44, step 2685, loss: 59536.273438\n",
      "epoch 44, step 2690, loss: 45492.164062\n",
      "epoch 44, step 2695, loss: 55359.679688\n",
      "Valid Weighted Classification Accuracy: 0.072300\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn.metrics as sk\n",
    "\n",
    "def eval_hyperparams(n_hidden=50, n_layers=1, learning_rate=.001,\n",
    "                           dropout_prob=0.5, n_epochs=45, batch_size=1000,\n",
    "                           weight_positives=True):\n",
    "    \n",
    "    print(\"---------------------------------------------\")\n",
    "    print(\"Model hyperparameters\")\n",
    "    print(\"n_hidden = %d\" % n_hidden)\n",
    "    print(\"n_layers = %d\" % n_layers)\n",
    "    print(\"learning_rate = %f\" % learning_rate)\n",
    "    print(\"n_epochs = %d\" % n_epochs)\n",
    "    print(\"batch_size = %d\" % batch_size)\n",
    "    print(\"weight_positives = %s\" % str(weight_positives))\n",
    "    print(\"dropout_prob = %f\" % dropout_prob)\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    d = 784\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        (train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "        # Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "        train_x = train_x.reshape(train_x.shape[0], 784)\n",
    "        test_x = test_x.reshape(test_x.shape[0], 784)\n",
    "\n",
    "        # Making sure that the values are float so that we can get decimal points after division\n",
    "        train_x = train_x.astype('float32')\n",
    "        test_x = test_x.astype('float32')\n",
    "        \n",
    "        # Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "        train_x /= 255\n",
    "        test_x /= 255\n",
    "\n",
    "        # Generate tensorflow graph\n",
    "        with tf.name_scope(\"placeholders\"):\n",
    "            x = tf.placeholder(tf.float32, (None, d))\n",
    "            y = tf.placeholder(tf.float32, (None,))\n",
    "            w = tf.placeholder(tf.float32, (None,))\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "        for layer in range(n_layers):\n",
    "            with tf.name_scope(\"layer-%d\" % layer):\n",
    "                W = tf.Variable(tf.random_normal((d, n_hidden)))\n",
    "                b = tf.Variable(tf.random_normal((n_hidden,)))\n",
    "                x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "                # Apply dropout\n",
    "                x_hidden = tf.nn.dropout(x_hidden, keep_prob)\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(tf.random_normal((n_hidden, 1)))\n",
    "            b = tf.Variable(tf.random_normal((1,)))\n",
    "            y_logit = tf.matmul(x_hidden, W) + b\n",
    "            # the sigmoid gives the class probability of 1\n",
    "            y_one_prob = tf.sigmoid(y_logit)\n",
    "            # Rounding P(y=1) will give the correct prediction.\n",
    "            y_pred = tf.round(y_one_prob)\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # Compute the cross-entropy term for each datapoint\n",
    "            y_expand = tf.expand_dims(y, 1)\n",
    "            entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
    "#             # Multiply by weights\n",
    "#             if weight_positives:\n",
    "#                 w_expand = tf.expand_dims(w, 1)\n",
    "#                 entropy = w_expand * entropy\n",
    "            # Sum all contributions\n",
    "            l = tf.reduce_sum(entropy**2)\n",
    "\n",
    "        with tf.name_scope(\"optim\"):\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "\n",
    "        hyperparam_str = \"d-%d-hidden-%d-lr-%f-n_epochs-%d-batch_size-%d-weight_pos-%s\" % (\n",
    "            d, n_hidden, learning_rate, n_epochs, batch_size, str(weight_positives))\n",
    "        N = train_x.shape[0]\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            step = 0\n",
    "            for epoch in range(n_epochs):\n",
    "                pos = 0\n",
    "                while pos < N:\n",
    "                    batch_X = train_x[pos:pos+batch_size]\n",
    "                    batch_y = train_y[pos:pos+batch_size]\n",
    "                    feed_dict = {x: batch_X, y: batch_y, keep_prob: dropout_prob}\n",
    "                    _, loss = sess.run([train_op, l], feed_dict=feed_dict)\n",
    "                    if (step % 5 == 0):\n",
    "                        print(\"epoch %d, step %d, loss: %f\" % (epoch, step, loss))\n",
    "                    step += 1\n",
    "                    pos += batch_size\n",
    "\n",
    "            # Make Predictions (set keep_prob to 1.0 for predictions)\n",
    "            test_y_pred = sess.run(y_pred, feed_dict={x: test_x, keep_prob: 1.0})\n",
    "\n",
    "        weighted_score = sk.accuracy_score(test_y, test_y_pred)\n",
    "        print(\"Valid Weighted Classification Accuracy: %f\" % weighted_score)\n",
    "    return weighted_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "      score = eval_hyperparams()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
