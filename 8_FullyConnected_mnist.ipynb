{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn.metrics as sk\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a3e888cf8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_index = 2049 # You may select anything up to 60,000\n",
    "print(y_train[image_index]) # The label is 5\n",
    "plt.imshow(x_train[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 784)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "input_shape = (28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 784 # 28*28 the size of an image\n",
    "n_hidden = 50\n",
    "learning_rate = .001\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "dropout_prob = 1.0\n",
    "\n",
    "with tf.name_scope(\"placeholders\"):\n",
    "    x = tf.placeholder(tf.float32, (None, d))\n",
    "    y = tf.placeholder(tf.float32, (None,))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "with tf.name_scope(\"hidden-layer\"):\n",
    "    W = tf.Variable(tf.random_normal((d, n_hidden)))\n",
    "    b = tf.Variable(tf.random_normal((n_hidden,)))\n",
    "    x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "    x_hidden = tf.nn.dropout(x_hidden, keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"output\"):\n",
    "    W = tf.Variable(tf.random_normal((n_hidden, 1)))\n",
    "    b = tf.Variable(tf.random_normal((1,)))\n",
    "    y_logit = tf.matmul(x_hidden, W) + b\n",
    "    # the sigmoid gives the class probability of 1\n",
    "    y_one_prob = tf.sigmoid(y_logit)\n",
    "    # Rounding P(y=1) will give the correct prediction.\n",
    "    y_pred = tf.round(y_one_prob)\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    # Compute the cross-entropy term for each datapoint\n",
    "    y_expand = tf.expand_dims(y, 1)\n",
    "    entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
    "    # Sum all contributions\n",
    "    l = tf.reduce_sum(entropy**2)\n",
    "\n",
    "with tf.name_scope(\"optim\"):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "\n",
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"loss\", l)\n",
    "    merged = tf.summary.merge_all()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, step 0, loss: 1525008.000000\n",
      "epoch 0, step 100, loss: 907200.000000\n",
      "epoch 0, step 200, loss: 376511.812500\n",
      "epoch 0, step 300, loss: 359591.781250\n",
      "epoch 0, step 400, loss: 220597.718750\n",
      "epoch 0, step 500, loss: 348927.625000\n",
      "epoch 1, step 600, loss: 176054.937500\n",
      "epoch 1, step 700, loss: 126411.156250\n",
      "epoch 1, step 800, loss: 94705.656250\n",
      "epoch 1, step 900, loss: 116547.906250\n",
      "epoch 1, step 1000, loss: 71465.515625\n",
      "epoch 1, step 1100, loss: 126313.750000\n",
      "epoch 2, step 1200, loss: 73727.320312\n",
      "epoch 2, step 1300, loss: 45896.601562\n",
      "epoch 2, step 1400, loss: 37905.656250\n",
      "epoch 2, step 1500, loss: 52692.535156\n",
      "epoch 2, step 1600, loss: 36448.308594\n",
      "epoch 2, step 1700, loss: 52007.265625\n",
      "epoch 3, step 1800, loss: 37791.910156\n",
      "epoch 3, step 1900, loss: 26987.953125\n",
      "epoch 3, step 2000, loss: 17772.511719\n",
      "epoch 3, step 2100, loss: 30749.078125\n",
      "epoch 3, step 2200, loss: 17714.123047\n",
      "epoch 3, step 2300, loss: 24753.500000\n",
      "epoch 4, step 2400, loss: 19804.109375\n",
      "epoch 4, step 2500, loss: 18131.179688\n",
      "epoch 4, step 2600, loss: 12005.583984\n",
      "epoch 4, step 2700, loss: 18381.960938\n",
      "epoch 4, step 2800, loss: 9560.271484\n",
      "epoch 4, step 2900, loss: 13877.796875\n",
      "epoch 5, step 3000, loss: 9674.119141\n",
      "epoch 5, step 3100, loss: 10634.404297\n",
      "epoch 5, step 3200, loss: 7316.009766\n",
      "epoch 5, step 3300, loss: 10584.300781\n",
      "epoch 5, step 3400, loss: 5305.354492\n",
      "epoch 5, step 3500, loss: 7530.397461\n",
      "epoch 6, step 3600, loss: 4717.759766\n",
      "epoch 6, step 3700, loss: 5850.474121\n",
      "epoch 6, step 3800, loss: 4336.292480\n",
      "epoch 6, step 3900, loss: 6069.753418\n",
      "epoch 6, step 4000, loss: 3043.277100\n",
      "epoch 6, step 4100, loss: 4051.548340\n",
      "epoch 7, step 4200, loss: 2406.989258\n",
      "epoch 7, step 4300, loss: 3100.746094\n",
      "epoch 7, step 4400, loss: 2423.291748\n",
      "epoch 7, step 4500, loss: 3593.854492\n",
      "epoch 7, step 4600, loss: 1781.412354\n",
      "epoch 7, step 4700, loss: 2341.984131\n",
      "epoch 8, step 4800, loss: 1192.872559\n",
      "epoch 8, step 4900, loss: 1627.360229\n",
      "epoch 8, step 5000, loss: 1299.544922\n",
      "epoch 8, step 5100, loss: 2208.564209\n",
      "epoch 8, step 5200, loss: 1033.288696\n",
      "epoch 8, step 5300, loss: 1228.475586\n",
      "epoch 9, step 5400, loss: 572.279419\n",
      "epoch 9, step 5500, loss: 922.658325\n",
      "epoch 9, step 5600, loss: 632.538452\n",
      "epoch 9, step 5700, loss: 1396.984741\n",
      "epoch 9, step 5800, loss: 602.563171\n",
      "epoch 9, step 5900, loss: 572.365479\n",
      "epoch 10, step 6000, loss: 256.864563\n",
      "epoch 10, step 6100, loss: 578.398132\n",
      "epoch 10, step 6200, loss: 299.655792\n",
      "epoch 10, step 6300, loss: 892.959229\n",
      "epoch 10, step 6400, loss: 376.674744\n",
      "epoch 10, step 6500, loss: 262.360443\n",
      "epoch 11, step 6600, loss: 112.850174\n",
      "epoch 11, step 6700, loss: 371.765137\n",
      "epoch 11, step 6800, loss: 144.308075\n",
      "epoch 11, step 6900, loss: 580.301392\n",
      "epoch 11, step 7000, loss: 255.005051\n",
      "epoch 11, step 7100, loss: 113.765427\n",
      "epoch 12, step 7200, loss: 53.546959\n",
      "epoch 12, step 7300, loss: 241.706787\n",
      "epoch 12, step 7400, loss: 69.534958\n",
      "epoch 12, step 7500, loss: 381.681580\n",
      "epoch 12, step 7600, loss: 183.818878\n",
      "epoch 12, step 7700, loss: 51.529839\n",
      "epoch 13, step 7800, loss: 31.740200\n",
      "epoch 13, step 7900, loss: 155.405670\n",
      "epoch 13, step 8000, loss: 37.521332\n",
      "epoch 13, step 8100, loss: 245.879974\n",
      "epoch 13, step 8200, loss: 138.322586\n",
      "epoch 13, step 8300, loss: 34.235935\n",
      "epoch 14, step 8400, loss: 23.777716\n",
      "epoch 14, step 8500, loss: 97.880249\n",
      "epoch 14, step 8600, loss: 25.268017\n",
      "epoch 14, step 8700, loss: 161.569901\n",
      "epoch 14, step 8800, loss: 103.940521\n",
      "epoch 14, step 8900, loss: 26.612598\n",
      "epoch 15, step 9000, loss: 20.571747\n",
      "epoch 15, step 9100, loss: 61.611656\n",
      "epoch 15, step 9200, loss: 23.117941\n",
      "epoch 15, step 9300, loss: 104.141464\n",
      "epoch 15, step 9400, loss: 79.212692\n",
      "epoch 15, step 9500, loss: 20.184864\n",
      "epoch 16, step 9600, loss: 18.888042\n",
      "epoch 16, step 9700, loss: 42.947540\n",
      "epoch 16, step 9800, loss: 19.827862\n",
      "epoch 16, step 9900, loss: 63.857307\n",
      "epoch 16, step 10000, loss: 62.710541\n",
      "epoch 16, step 10100, loss: 19.327436\n",
      "epoch 17, step 10200, loss: 17.884533\n",
      "epoch 17, step 10300, loss: 31.639893\n",
      "epoch 17, step 10400, loss: 18.727814\n",
      "epoch 17, step 10500, loss: 38.780220\n",
      "epoch 17, step 10600, loss: 48.653481\n",
      "epoch 17, step 10700, loss: 24.127504\n",
      "epoch 18, step 10800, loss: 17.464552\n",
      "epoch 18, step 10900, loss: 23.876530\n",
      "epoch 18, step 11000, loss: 17.903568\n",
      "epoch 18, step 11100, loss: 26.446726\n",
      "epoch 18, step 11200, loss: 38.559273\n",
      "epoch 18, step 11300, loss: 22.909304\n",
      "epoch 19, step 11400, loss: 17.140930\n",
      "epoch 19, step 11500, loss: 20.270037\n",
      "epoch 19, step 11600, loss: 18.352306\n",
      "epoch 19, step 11700, loss: 19.877087\n",
      "epoch 19, step 11800, loss: 31.640614\n",
      "epoch 19, step 11900, loss: 19.783701\n",
      "epoch 20, step 12000, loss: 16.959267\n",
      "epoch 20, step 12100, loss: 18.106619\n",
      "epoch 20, step 12200, loss: 19.024845\n",
      "epoch 20, step 12300, loss: 17.346523\n",
      "epoch 20, step 12400, loss: 25.536335\n",
      "epoch 20, step 12500, loss: 18.854149\n",
      "epoch 21, step 12600, loss: 16.793070\n",
      "epoch 21, step 12700, loss: 17.486336\n",
      "epoch 21, step 12800, loss: 18.011311\n",
      "epoch 21, step 12900, loss: 16.926220\n",
      "epoch 21, step 13000, loss: 21.256216\n",
      "epoch 21, step 13100, loss: 16.185570\n",
      "epoch 22, step 13200, loss: 16.734020\n",
      "epoch 22, step 13300, loss: 17.170315\n",
      "epoch 22, step 13400, loss: 17.835426\n",
      "epoch 22, step 13500, loss: 16.631237\n",
      "epoch 22, step 13600, loss: 18.579468\n",
      "epoch 22, step 13700, loss: 14.966210\n",
      "epoch 23, step 13800, loss: 16.570862\n",
      "epoch 23, step 13900, loss: 17.372044\n",
      "epoch 23, step 14000, loss: 17.775711\n",
      "epoch 23, step 14100, loss: 16.288857\n",
      "epoch 23, step 14200, loss: 17.115053\n",
      "epoch 23, step 14300, loss: 14.652303\n",
      "epoch 24, step 14400, loss: 16.231869\n",
      "epoch 24, step 14500, loss: 17.165106\n",
      "epoch 24, step 14600, loss: 17.438932\n",
      "epoch 24, step 14700, loss: 15.968407\n",
      "epoch 24, step 14800, loss: 16.255520\n",
      "epoch 24, step 14900, loss: 14.416832\n",
      "epoch 25, step 15000, loss: 15.853510\n",
      "epoch 25, step 15100, loss: 16.744963\n",
      "epoch 25, step 15200, loss: 16.730740\n",
      "epoch 25, step 15300, loss: 15.479568\n",
      "epoch 25, step 15400, loss: 15.961711\n",
      "epoch 25, step 15500, loss: 14.161549\n",
      "epoch 26, step 15600, loss: 15.438623\n",
      "epoch 26, step 15700, loss: 16.057911\n",
      "epoch 26, step 15800, loss: 15.990079\n",
      "epoch 26, step 15900, loss: 14.903834\n",
      "epoch 26, step 16000, loss: 15.724974\n",
      "epoch 26, step 16100, loss: 13.910292\n",
      "epoch 27, step 16200, loss: 14.959166\n",
      "epoch 27, step 16300, loss: 15.355049\n",
      "epoch 27, step 16400, loss: 15.298357\n",
      "epoch 27, step 16500, loss: 14.347876\n",
      "epoch 27, step 16600, loss: 15.401753\n",
      "epoch 27, step 16700, loss: 13.587009\n",
      "epoch 28, step 16800, loss: 14.586983\n",
      "epoch 28, step 16900, loss: 14.729849\n",
      "epoch 28, step 17000, loss: 14.615209\n",
      "epoch 28, step 17100, loss: 13.801431\n",
      "epoch 28, step 17200, loss: 15.142454\n",
      "epoch 28, step 17300, loss: 13.171543\n",
      "epoch 29, step 17400, loss: 14.183989\n",
      "epoch 29, step 17500, loss: 14.014333\n",
      "epoch 29, step 17600, loss: 14.218225\n",
      "epoch 29, step 17700, loss: 13.409408\n",
      "epoch 29, step 17800, loss: 15.021378\n",
      "epoch 29, step 17900, loss: 12.895723\n",
      "epoch 30, step 18000, loss: 13.659615\n",
      "epoch 30, step 18100, loss: 13.411062\n",
      "epoch 30, step 18200, loss: 13.501820\n",
      "epoch 30, step 18300, loss: 13.119263\n",
      "epoch 30, step 18400, loss: 14.830710\n",
      "epoch 30, step 18500, loss: 12.324372\n",
      "epoch 31, step 18600, loss: 12.899372\n",
      "epoch 31, step 18700, loss: 12.409489\n",
      "epoch 31, step 18800, loss: 12.487347\n",
      "epoch 31, step 18900, loss: 13.009119\n",
      "epoch 31, step 19000, loss: 14.342277\n",
      "epoch 31, step 19100, loss: 10.884318\n",
      "epoch 32, step 19200, loss: 11.735186\n",
      "epoch 32, step 19300, loss: 11.569479\n",
      "epoch 32, step 19400, loss: 11.207452\n",
      "epoch 32, step 19500, loss: 11.681597\n",
      "epoch 32, step 19600, loss: 13.481277\n",
      "epoch 32, step 19700, loss: 10.154871\n",
      "epoch 33, step 19800, loss: 11.654984\n",
      "epoch 33, step 19900, loss: 10.800395\n",
      "epoch 33, step 20000, loss: 10.493555\n",
      "epoch 33, step 20100, loss: 11.458114\n",
      "epoch 33, step 20200, loss: 12.507324\n",
      "epoch 33, step 20300, loss: 9.783892\n",
      "epoch 34, step 20400, loss: 10.860822\n",
      "epoch 34, step 20500, loss: 10.054230\n",
      "epoch 34, step 20600, loss: 9.753555\n",
      "epoch 34, step 20700, loss: 11.180037\n",
      "epoch 34, step 20800, loss: 11.999422\n",
      "epoch 34, step 20900, loss: 9.860943\n",
      "epoch 35, step 21000, loss: 10.374863\n",
      "epoch 35, step 21100, loss: 9.453415\n",
      "epoch 35, step 21200, loss: 8.847803\n",
      "epoch 35, step 21300, loss: 10.772123\n",
      "epoch 35, step 21400, loss: 11.602329\n",
      "epoch 35, step 21500, loss: 8.886082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36, step 21600, loss: 10.097156\n",
      "epoch 36, step 21700, loss: 9.042549\n",
      "epoch 36, step 21800, loss: 7.704368\n",
      "epoch 36, step 21900, loss: 10.050305\n",
      "epoch 36, step 22000, loss: 11.153442\n",
      "epoch 36, step 22100, loss: 7.989295\n",
      "epoch 37, step 22200, loss: 9.785727\n",
      "epoch 37, step 22300, loss: 8.424888\n",
      "epoch 37, step 22400, loss: 6.964948\n",
      "epoch 37, step 22500, loss: 9.366642\n",
      "epoch 37, step 22600, loss: 9.685022\n",
      "epoch 37, step 22700, loss: 6.365896\n",
      "epoch 38, step 22800, loss: 8.174784\n",
      "epoch 38, step 22900, loss: 6.659739\n",
      "epoch 38, step 23000, loss: 6.391805\n",
      "epoch 38, step 23100, loss: 7.658178\n",
      "epoch 38, step 23200, loss: 7.248374\n",
      "epoch 38, step 23300, loss: 5.199119\n",
      "epoch 39, step 23400, loss: 6.946386\n",
      "epoch 39, step 23500, loss: 6.122515\n",
      "epoch 39, step 23600, loss: 6.032471\n",
      "epoch 39, step 23700, loss: 7.270615\n",
      "epoch 39, step 23800, loss: 6.008629\n",
      "epoch 39, step 23900, loss: 4.662109\n",
      "epoch 40, step 24000, loss: 6.514796\n",
      "epoch 40, step 24100, loss: 5.717005\n",
      "epoch 40, step 24200, loss: 5.688677\n",
      "epoch 40, step 24300, loss: 6.690426\n",
      "epoch 40, step 24400, loss: 5.184796\n",
      "epoch 40, step 24500, loss: 4.413946\n",
      "epoch 41, step 24600, loss: 6.150401\n",
      "epoch 41, step 24700, loss: 5.489929\n",
      "epoch 41, step 24800, loss: 5.362730\n",
      "epoch 41, step 24900, loss: 6.687603\n",
      "epoch 41, step 25000, loss: 4.802608\n",
      "epoch 41, step 25100, loss: 4.257079\n",
      "epoch 42, step 25200, loss: 5.886461\n",
      "epoch 42, step 25300, loss: 5.314379\n",
      "epoch 42, step 25400, loss: 5.300856\n",
      "epoch 42, step 25500, loss: 6.075768\n",
      "epoch 42, step 25600, loss: 4.504581\n",
      "epoch 42, step 25700, loss: 3.975364\n",
      "epoch 43, step 25800, loss: 5.595945\n",
      "epoch 43, step 25900, loss: 5.060516\n",
      "epoch 43, step 26000, loss: 4.842953\n",
      "epoch 43, step 26100, loss: 6.185451\n",
      "epoch 43, step 26200, loss: 4.078478\n",
      "epoch 43, step 26300, loss: 3.892315\n",
      "epoch 44, step 26400, loss: 5.364421\n",
      "epoch 44, step 26500, loss: 4.899479\n",
      "epoch 44, step 26600, loss: 4.626580\n",
      "epoch 44, step 26700, loss: 5.694915\n",
      "epoch 44, step 26800, loss: 3.747761\n",
      "epoch 44, step 26900, loss: 3.735310\n",
      "epoch 45, step 27000, loss: 5.087156\n",
      "epoch 45, step 27100, loss: 4.790613\n",
      "epoch 45, step 27200, loss: 4.420948\n",
      "epoch 45, step 27300, loss: 5.923248\n",
      "epoch 45, step 27400, loss: 3.641429\n",
      "epoch 45, step 27500, loss: 3.753714\n",
      "epoch 46, step 27600, loss: 4.818410\n",
      "epoch 46, step 27700, loss: 4.659066\n",
      "epoch 46, step 27800, loss: 4.293112\n",
      "epoch 46, step 27900, loss: 5.413455\n",
      "epoch 46, step 28000, loss: 3.255124\n",
      "epoch 46, step 28100, loss: 3.738472\n",
      "epoch 47, step 28200, loss: 4.675932\n",
      "epoch 47, step 28300, loss: 4.501355\n",
      "epoch 47, step 28400, loss: 4.153965\n",
      "epoch 47, step 28500, loss: 5.504247\n",
      "epoch 47, step 28600, loss: 3.344555\n",
      "epoch 47, step 28700, loss: 3.699164\n",
      "epoch 48, step 28800, loss: 4.482601\n",
      "epoch 48, step 28900, loss: 4.414237\n",
      "epoch 48, step 29000, loss: 4.073977\n",
      "epoch 48, step 29100, loss: 5.107605\n",
      "epoch 48, step 29200, loss: 2.986919\n",
      "epoch 48, step 29300, loss: 3.685473\n",
      "epoch 49, step 29400, loss: 4.459275\n",
      "epoch 49, step 29500, loss: 4.261030\n",
      "epoch 49, step 29600, loss: 3.992849\n",
      "epoch 49, step 29700, loss: 5.171230\n",
      "epoch 49, step 29800, loss: 2.940253\n",
      "epoch 49, step 29900, loss: 3.659677\n",
      "Train Weighted Classification Accuracy: 0.193000\n",
      "Valid Weighted Classification Accuracy: 0.194300\n"
     ]
    }
   ],
   "source": [
    "N = x_train.shape[0]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 0\n",
    "    print(n_epochs)\n",
    "    for epoch in range(n_epochs):\n",
    "        pos = 0\n",
    "        while pos < N:\n",
    "            batch_X = x_train[pos:pos+batch_size]\n",
    "            batch_y = y_train[pos:pos+batch_size]\n",
    "            feed_dict = {x: batch_X, y: batch_y, keep_prob: dropout_prob}\n",
    "            _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "            if (step % 100 == 0):\n",
    "                print(\"epoch %d, step %d, loss: %f\" % (epoch, step, loss))\n",
    "            step += 1\n",
    "            pos += batch_size\n",
    "    \n",
    "    # Make Predictions (set keep_prob to 1.0 for predictions)\n",
    "    train_y_pred = sess.run(y_pred, feed_dict={x: x_train, keep_prob: 1.0})\n",
    "    test_y_pred = sess.run(y_pred, feed_dict={x: x_test, keep_prob: 1.0})\n",
    "\n",
    "    train_weighted_score = sk.accuracy_score(y_train, train_y_pred)\n",
    "    print(\"Train Weighted Classification Accuracy: %f\" % train_weighted_score)\n",
    "    test_weighted_score = sk.accuracy_score(y_test, test_y_pred)\n",
    "    print(\"Valid Weighted Classification Accuracy: %f\" % test_weighted_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
